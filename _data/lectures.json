{"itp-S19":[
	{
		"title": "Art, autonomy, and AI",
		"date": "31 Jan 2019",
		"main":"/classes/itp-S19/01/",
		"slides": "https://storage.googleapis.com/ml4a.github.io/AAA/Slides_01/index.html",
		"bookmarks": [],
		"summary": [
			"The idea of the artificial autonomous artist",
			"The whole class \"in 60 minutes\""
		],
		"practical": [],
		"extra": []
	},
	{
		"title": "Multiplayer games & crowd-sourced art",
		"date": "7 Feb 2019",
		"main":"/classes/itp-S19/02/",
		"bookmarks": [],
		"summary": [
			"Artificial life and early computational arts",
			"The hive mind: crowd-sourced art projects"
		],
		"practical": [],
		"extra": []
	},
	{
		"title": "Generative models",
		"date": "14 Feb 2019",
		"main":"/classes/itp-S19/03/",
		"bookmarks": [],
		"summary": [
			"GANs, autoencoders, oh my",
			"Multiplexing deep generative models"
		],
		"practical": [],
		"extra": []
	},
	{
		"title": "Decentralized AI",
		"date": "21 Feb 2019",
		"main":"/classes/itp-S19/04/",
		"bookmarks": [],
		"summary": [
			"Taking the machine out of machine learning",
			"Data and compute markets",
			"OpenMined, TrueBit, and Ocean protocols"
		],
		"practical": [],
		"extra": []
	},
	{
		"title": "The collective imagination I",
		"date": "28 Feb 2019",
		"main":"/classes/itp-S19/05/",
		"bookmarks": [],
		"summary": [
			"Putting the pieces together"
		],
		"practical": [],
		"extra": []
	},
	{
		"title": "The collective imagination II",
		"date": "7 Mar 2019",
		"main":"/classes/itp-S19/06/",
		"bookmarks": [],
		"summary": [
			"Putting the pieces together"
		],
		"practical": [],
		"extra": []
	},
	{
		"title": "Genesis",
		"date": "14 Mar 2019",
		"main":"/classes/itp-S19/07/",
		"bookmarks": [],
		"summary": [
			"Summoning our AAA"
		],
		"practical": [],
		"extra": []
	}
],
"itp-F18":[
	{
		"title": "The whole class \"in 60 minutes\"",
		"date": "10 Sep 2018",
		"youtube_id": "Bh5zb-IssWQ",
		"main":"/classes/itp-F18/01/",
		"slides": "https://storage.googleapis.com/ml4a.github.io/Slides_01/index.html",
		"thumbnail": "/images/classes/itp-F18/thumbnail_01.png",
		"bookmarks": [
			{"title":"Introduction", "m":0, "s":0, "disp":"0:00"},
			{"title":"Course logistics", "m":3, "s":21, "disp":"3:21"}, 
			{"title":"Syllabus", "m":8, "s":13, "disp":"8:13"}, 
			{"title":"Tools and frameworks", "m":16, "s":43, "disp":"16:43"}, 
			{"title":"Introduction to machine learning", "m":27, "s":06, "disp":"27:06"}, 
			{"title":"AI resurgence and deep learning", "m":36, "s":36, "disp":"36:36"}, 
			{"title":"Characteristics of deep learning", "m":47, "s":11, "disp":"47:11"}, 
			{"title":"Types of machine learning", "m":51, "s":22, "disp":"51:22"}, 
			{"title":"Supervised learning", "m":55, "s":14, "disp":"55:14"}, 
			{"title":"Examples of supervised learning", "m":62, "s":42, "disp":"1:02:42"}, 
			{"title":"Neural networks and feature extraction", "m":75, "s":28, "disp":"1:15:28"}, 
			{"title":"Core applications of feature extraction", "m":78, "s":20, "disp":"1:18:20"}, 
			{"title":"Interactive machine learning", "m":84, "s":40, "disp":"1:24:40"}, 
			{"title":"Deepdream, style transfer, and texture synthesis", "m":91, "s":40, "disp":"1:31:40"}, 
			{"title":"Generative models", "m":96, "s":19, "disp":"1:36:19"}, 
			{"title":"Conditional generative models (Image-to-image)", "m":106, "s":18, "disp":"1:46:18"}, 
			{"title":"Voice synthesis, language models, and miscellaneous", "m":121, "s":30, "disp":"2:01:30"}, 
			{"title":"Reinforcement learning and decentralized AI", "m":127, "s":35, "disp":"2:07:35"}
		],
		"summary": [
			"Course goals, logistics, and resources",
			"Introduction to AI, machine learning, and deep learning",
			"The whole class \"in 60 minutes\""
		],
		"pratical": [
			"<a href=\"https://ml5js.org/\">ml5.js</a>",
			"<a href=\"https://github.com/ml4a/ml4a-demos\">ml4a-demos</a>",
			"<a href=\"https://runwayml.com/\">Runway</a>",
			"<a href=\"https://github.com/ml4a/ml4a-ofx\">ml4a-ofx</a>"
		],
		"extra": [
			"<a href=\"https://ml4a.github.io/guides/\">ml4a-guides</a>",
			"<a href=\"https://experiments.withgoogle.com/collection/ai\">AI experiments</a>"
		]	
	},
	{
		"title": "Neural networks",
		"date": "11 Sep 2018",
		"youtube_id": "FYqsA2B-jKE",
		"main":"/classes/itp-F18/02/",
		"slides": "https://storage.googleapis.com/ml4a.github.io/Slides_02/index.html",
		"thumbnail": "/images/classes/itp-F18/thumbnail_02.png",
		"bookmarks": [
			{"title":"Admin", "m":0, "s":0, "disp":"0:00"},
			{"title":"What are features", "m":8, "s":0, "disp":"8:00"},
			{"title":"Neurons and linear functions", "m":16, "s":23, "disp":"16:23"},
			{"title":"Non-linearities and hidden layers", "m":20, "s":23, "disp":"20:23"},
			{"title":"Demo of a simple forward pass", "m":24, "s":49, "disp":"24:49"},
			{"title":"ReLU activations", "m":38, "s":44, "disp":"38:44"},
			{"title":"Neural net for classifying MNIST", "m":40, "s":19, "disp":"40:19"},
			{"title":"Visualizing the weights", "m":48, "s":36, "disp":"48:36"},
			{"title":"Visualizing weights of hidden layers", "m":59, "s":32, "disp":"59:32"},
			{"title":"A more complicated dataset: CIFAR-10", "m":66, "s":55, "disp":"1:06:55"},
			{"title":"Precursors to convolutional networks", "m":70, "s":27, "disp":"1:10:27"},
			{"title":"How convolutional layers work", "m":82, "s":55, "disp":"1:22:55"},
			{"title":"Full demo of a convolutional network", "m":92, "s":37, "disp":"1:32:37"},
			{"title":"Impact of convolutional networks", "m":109, "s":11, "disp":"1:49:11"},
			{"title":"ml5.js image classifier", "m":114, "s":55, "disp":"1:54:55"},
			{"title":"Transfer learning", "m":123, "s":26, "disp":"2:03:26"},
			{"title":"ml5.js transfer learning demos", "m":130, "s":58, "disp":"2:10:58"},
			{"title":"Transfer learning creative projects", "m":140, "s":34, "disp":"2:20:34"}
		],
		"summary": [
			"How convolutional neural networks work (forward pass)",
			"Transfer learning",
			"Demos and applications of transfer learning"
		],
		"pratical": [
			"<a href=\"https://github.com/ml4a/ml4a-guides/blob/master/notebooks/convolutional_neural_networks.ipynb\">Keras image classification</a>",
			"<a href=\"https://ml5js.org/docs/ImageClassifier\">ml5 live image classifier</a>"
		],
		"extra": [
			"<a href=\"https://ml4a.github.io/ml4a/neural_networks/\">Intro to neural networks</a>",
			"<a href=\"http://ml4a.github.io/ml4a/convnets/\">Convolutional networks</a>"
		]
	},
	{
		"title": "How neural nets are trained",
		"date": "18 Sep 2018",
		"youtube_id": "Ger4Y9Ovrb4",
		"main":"/classes/itp-F18/03/",
		"slides": "https://storage.googleapis.com/ml4a.github.io/Slides_03/index.html",
		"thumbnail": "/images/classes/itp-F18/thumbnail_03.png",
		"bookmarks": [
			{"title":"Introduction, announcements", "m":0, "s":0, "disp":"0:00"},
			{"title":"Review of supervised learning pipeline", "m":5, "s":41, "disp":"5:41"}, 
			{"title":"Why training is hard", "m":10, "s":06, "disp":"10:06"}, 
			{"title":"Linear regression", "m":15, "s":33, "disp":"15:33"}, 
			{"title":"Gradient descent", "m":20, "s":18, "disp":"20:18"}, 
			{"title":"Calculating the gradient, backpropagation", "m":31, "s":32, "disp":"31:32"}, 
			{"title":"The problem of non-convexity, SGD, and mini-batches", "m":38, "s":31, "disp":"38:31"}, 
			{"title":"Momentum and adaptive optimizers", "m":44, "s":29, "disp":"44:29"},
			{"title":"Overfitting and regularization, dropout", "m":48, "s":52, "disp":"48:52"}, 
			{"title":"Further reading & questions", "m":56, "s":01, "disp":"56:01"}, 
			{"title":"Overview of ml4a-ofx", "m":63, "s":40, "disp":"1:03:40"}, 
			{"title":"Demo of ConvnetPredictor (webcam transfer learning)", "m":68, "s":12, "disp":"1:08:12"}, 
			{"title":"Communicating between ConvnetPredictor and Processing", "m":79, "s":36, "disp":"1:19:36"},
			{"title":"Regression controlling generative art sketch", "m":92, "s":53, "disp":"1:32:53"}, 
			{"title":"Controlling Ableton Live with ConvnetPredictor", "m":100, "s":24, "disp":"1:40:24"}, 
			{"title":"Demo of DoodleClassifier", "m":108, "s":48, "disp":"1:48:48"}, 
			{"title":"Demo of AudioClassifier and controlling the keyboard with sound (offline)", "m":119, "s":27, "disp":"1:59:27"}, 
			{"title":"Summary and comparison of tools", "m":128, "s":35, "disp":"2:08:35"}
		],
		"summary": [
			"How neural nets are trained (backward pass)",
			"Overfitting, regularization, optimization",
			"ml4a-ofx demos: ConvnetPredictor, AudioClassifier, DoodleClassifier"
		],
		"pratical": [
			"<a href=\"https://ml4a.github.io/guides/DoodleClassifier\">Doodle Classifier</a>",
			"<a href=\"https://ml4a.github.io/guides/ConvnetPredictor\">ConvnetPredictor</a> / <a href=\"https://ml4a.github.io/guides/ConvnetOSC\">ConvnetOSC</a>",
			"<a href=\"https://ml4a.github.io/guides/AudioClassifier\">AudioClassifier</a>"
		],
		"extra": [
			"<a href=\"https://ml4a.github.io/ml4a/how_neural_networks_are_trained/\">How neural nets are trained</a>",
			"Numpy for ninjas: linear algebra [planned]"
		]
	},
	{
		"title": "Applications of neural nets",
		"date": "25 Sep 2018",
		"youtube_id": "zOZYCuDvJ3I",
		"main":"/classes/itp-F18/04/",
		"slides": "https://storage.googleapis.com/ml4a.github.io/Slides_04/index.html",
		"thumbnail": "/images/classes/itp-F18/thumbnail_04.png",
		"bookmarks": [
			{"title":"Announcements", "m":0, "s":0, "disp":"0:00"},
			{"title":"Deep learning frameworks", "m":7, "s":23, "disp":"7:23"},
			{"title":"Feature vectors", "m":12, "s":18, "disp":"12:18"},
			{"title":"Embeddings", "m":17, "s":35, "disp":"17:35"},
			{"title":"Review of neural nets, gradient descent, & feature extraction", "m":21, "s":17, "disp":"21:17"},
			{"title":"Word vectors & properties of language embeddings", "m":30, "s":45, "disp":"30:45"},
			{"title":"Sentence/paragraph vectors, language models & story generation", "m":35, "s":24, "disp":"35:24"},
			{"title":"Applications of feature vectors", "m":50, "s":25, "disp":"50:25"},
			{"title":"Introduction to ml4a-guides & Jupyter Lab", "m":66, "s":55, "disp":"1:06:55"},
			{"title":"Tutorial: image feature extraction", "m":84, "s":33, "disp":"1:24:33"},
			{"title":"Tutorial: reverse image search", "m":108, "s":13, "disp":"1:48:13"},
			{"title":"Tutorial: shortest path (X degrees of separation)", "m":118, "s":49, "disp":"1:58:49"},
			{"title":"t-SNE & data dataset visualization", "m":128, "s":15, "disp":"2:08:15"},
			{"title":"Tutorial: t-SNE of images", "m":133, "s":22, "disp":"2:13:22"},
			{"title":"Organizing sound clips with t-SNE", "m":139, "s":30, "disp":"2:19:30"},
			{"title":"Tutorial: audio t-SNE", "m":142, "s":35, "disp":"2:22:35"},
			{"title":"Interacting with t-SNE viewer (openFrameworks)", "m":147, "s":15, "disp":"2:27:15"},
			{"title":"ml5-examples (YOLO) & JavaScript demos", "m":152, "s":9, "disp":"2:32:09"},
			{"title":"Summary of practical materials & about mid-term mini-presentation", "m":155, "s":14, "disp":"2:35:14"}
		],
		"summary": [
			"Feature extraction",
			"Reverse image search and X degrees",
			"t-SNE of images/sounds & visualization",
			"Transfer learning"
		],
		"pratical": [
			"<a href=\"https://github.com/ml4a/ml4a-guides/tree/master/notebooks/image-path.ipynb\">Shortest path between images</a>",
			"<a href=\"https://github.com/ml4a/ml4a-guides/tree/master/notebooks/transfer-learning.ipynb\">Transfer learning</a>",
			"<a href=\"https://github.com/ml4a/ml4a-guides/tree/master/notebooks/image-tsne.ipynb\">t-SNE/UMAP of images</a>"
		],
		"extra": [
			"<a href=\"http://ml4a.github.io/ml4a/looking_inside_neural_nets/\">Looking inside neural nets</a>",
			"<a href=\"https://artsexperiments.withgoogle.com/xdegrees/\">X degrees of separation</a>"
		]
	},
	{
		"title": "Visualization, deepdream, style & texture synthesis",
		"date": "16 Oct 2018",
		"youtube_id": "9Ql0xyXQLt8",
		"main":"/classes/itp-F18/05/",
		"slides": "https://storage.googleapis.com/ml4a.github.io/Slides_05/index.html",
		"thumbnail": "/images/classes/itp-F18/thumbnail_05.png",
		"bookmarks": [
			{"title":"Plan for rest of the term", "m":0, "s":0, "disp":"0:00"},
			{"title":"A review of representation learning", "m":7, "s":2, "disp":"7:02"},
			{"title":"Visualizing convnet features", "m":12, "s":46, "disp":"12:46"},
			{"title":"Activating features via pixel optimization", "m":21, "s":38, "disp":"21:38"},
			{"title":"Inceptionism & Deepdream", "m":27, "s":47, "disp":"27:47"},
			{"title":"Masking deepdreams", "m":38, "s":52, "disp":"38:52"},
			{"title":"Style transfer", "m":46, "s":20, "disp":"46:20"},
			{"title":"Texture synthesis", "m":57, "s":13, "disp":"57:13"},
			{"title":"Miscellaneous optimization-based work", "m":59, "s":41, "disp":"59:41"}
		],
		"summary": [
			"Visualizing convnet features",
			"Pixel-optimization, Deepdream, neural-synth",
			"Style transfer and textur synthesis"
		],
		"pratical": [
			"<a href=\"https://ml4a.github.io/classes/itp-F18/terminal-velocity\">Terminal Velocity</a>"
		],
		"extra": [
			"<a href=\"https://colab.research.google.com/drive/1xeJAhTEwI3TNH_CJnTMq5AJuPkOs8sJ6\">Deepdream</a> + <a href=\"https://github.com/tensorflow/lucid\">Lucid</a>",
			"<a href=\"https://distill.pub/2017/feature-visualization/\">distill: Feature Visualization</a>",
			"<a href=\"https://distill.pub/2018/building-blocks/\">The Building Blocks of Interpretability</a>",
			"<a href=\"https://www.youtube.com/watch?v=AgkfIQ4IGaM\">Deep Visualization Toolbox</a>"
		]
	},
	{
		"title": "Terminal velocity",
		"date": "17 Oct 2018",
		"youtube_id": "cq4Nm2gup-U",
		"main":"/classes/itp-F18/terminal-velocity/",
		"thumbnail": "/images/classes/itp-F18/thumbnail_terminal-velocity.png",
		"bookmarks": [
			{"title":"Basic terminal commands", "m":3, "s":42, "disp":"3:42"},
			{"title":"Python shell, bash scripting, nohup", "m":9, "s":43, "disp":"9:43"},
			{"title":"Launching a remote VM on Paperspace", "m":16, "s":51, "disp":"16:51"},
			{"title":"Interfacing with your VM, launching Jupyter", "m":22, "s":0, "disp":"22:00"},
			{"title":"Jupyter Lab", "m":29, "s":50, "disp":"29:50"},
			{"title":"Setting up neural-style", "m":34, "s":14, "disp":"34:14"},
			{"title":"Running a style transfer", "m":37, "s":53, "disp":"37:53"},
			{"title":"Texture synthesis", "m":48, "s":18, "disp":"48:18"},
			{"title":"Setting up ml4a-guides and neural-synth", "m":50, "s":28, "disp":"50:28"},
			{"title":"Running basic deepdream", "m":51, "s":16, "disp":"51:16"},
			{"title":"Masking dreams, canvas distortion", "m":59, "s":23, "disp":"59:23"},
			{"title":"Generating videos", "m":67, "s":16, "disp":"1:07:16"}
		],
		"summary": [
			"Basic terminal navigation",
			"Python shell and bash scripting",
			"Connecting to remote VM"
		],
		"pratical": [
			"<a href=\"https://github.com/ml4a/ml4a-guides/blob/master/notebooks/neural-synth.ipynb\">neural-synth</a>",
			"<a href=\"https://github.com/jcjohnson/neural-style\">neural-style</a>"
		],
		"extra": [
			"<a href=\"https://github.com/tensorflow/lucid\">lucid</a>",
			"<a href=\"https://distill.pub/2018/differentiable-parameterizations/\">Differentiable Image Parameterizations</a>"
		]
	},
	{
		"date": "23 Oct 2018",
		"title": "Generative models",
		"youtube_id": "U_u5CuLGVSc",
		"main":"/classes/itp-F18/06/",
		"slides": "https://storage.googleapis.com/ml4a.github.io/Slides_06/index.html",
		"thumbnail": "/images/classes/itp-F18/thumbnail_06.png",
		"bookmarks": [
			{"title":"Why study generative models?", "m":3, "s":29, "disp":"3:29"},
			{"title":"What are generative models?", "m":8, "s":26, "disp":"8:26"},
			{"title":"Dimensionality reduction and PCA demo", "m":12, "s":15, "disp":"12:15"},
			{"title":"Pixel-space and the curse of dimensionality", "m":21, "s":57, "disp":"21:57"},
			{"title":"Eigenfaces", "m":29, "s":46, "disp":"29:46"},
			{"title":"Linear PCA vs Non-linear methods", "m":45, "s":25, "disp":"45:25"},
			{"title":"Neural net & embeddings review", "m":51, "s":37, "disp":"51:37"},
			{"title":"Autoencoders", "m":56, "s":26, "disp":"56:26"},
			{"title":"Generative adversarial networks", "m":70, "s":04, "disp":"1:10:04"},
			{"title":"DCGANs and feature arithmetic", "m":75, "s":, "disp":"1:15:30"},
			{"title":"DCGAN examples projects", "m":79, "s":35, "disp":"1:19:35"},
			{"title":"Deep generator networks", "m":91, "s":, "disp":"1:31:19"},
			{"title":"High-resolution and progressively-grown GANs", "m":94, "s":37, "disp":"1:34:37"},
			{"title":"GLOW and reversibility, fMRI-conditioned GANs", "m":100, "s":53, "disp":"1:40:53"},
			{"title":"Generative models in text and audio domain", "m":105, "s":47, "disp":"1:45:47"},
			{"title":"Practical resources and tutorials", "m":113, "s":39, "disp":"1:53:39"},
			{"title":"Scraping Instagram, Google, and Bing", "m":115, "s":0, "disp":"1:55:00"},
			{"title":"Finding publicly available datasets", "m":123, "s":42, "disp":"2:03:42"},
			{"title":"Dataset utils for pre-processing datasets", "m":127, "s":35, "disp":"2:07:35"},
			{"title":"Setting up Paperspace job-runner for DCGAN", "m":138, "s":9, "disp":"2:18:09"},
			{"title":"Training DCGAN-tensorflow", "m":147, "s":7, "disp":"2:27:07"},
			{"title":"Other GAN-training resources", "m":158, "s":46, "disp":"2:38:46"}
		],
		"summary": [
			"Autoencoders & Generative adversarial networks",
			"Compiling, scraping, and processing datasets",
			"Training DCGANs"
		],
		"pratical": [
			"<a href=\"https://github.com/ml4a/ml4a-guides/tree/master/notebooks/eigenfaces.ipynb\">Eigenfaces</a>",
			"<a href=\"https://github.com/carpedm20/DCGAN-tensorflow\">DCGAN-tensorflow</a>",
			"<a href=\"https://github.com/ml4a/ml4a-guides/tree/master/utils\">dataset utils</a>",
			"<a href=\"https://github.com/montoyamoraga/scrapers\">scrapers</a>"
		],
		"extra": [
			"Garbage in, treasure out: a field guide to compiling datasets on a messy internet [planned]",
			"<a href=\"http://ml4a.github.io/classes/misc/01/\">From PCA to Puppyslugs</a>"
		]
	},
	{
		"date": "30 Oct 2018",
		"title": "Conditional generative models",
		"youtube_id": "ZiQNI_UwNbg",
		"main":"/classes/itp-F18/07/",
		"slides": "https://storage.googleapis.com/ml4a.github.io/Slides_07/index.html",
		"thumbnail": "/images/classes/itp-F18/thumbnail_07.png",
		"bookmarks": [
			{"title":"Review of generative models", "m":5, "s":58, "disp":"5:58"},
			{"title":"Conditioning generative models", "m":18, "s":05, "disp":"18:05"},
			{"title":"Image-to-image translation (pix2pix)", "m":25, "s":34, "disp":"25:34"},
			{"title":"pix2pix projects", "m":29, "s":30, "disp":"29:30"},
			{"title":"Conditioning on face landmarks", "m":37, "s":54, "disp":"37:54"},
			{"title":"Conditioning on pose", "m":44, "s":13, "disp":"44:13"},
			{"title":"Drawing interface for pix2pix", "m":50, "s":54, "disp":"50:54"},
			{"title":"pix2pix ping-ponging and feedback loops", "m":51, "s":50, "disp":"51:50"},
			{"title":"Interactive interfaces and edge2landscapes", "m":57, "s":47, "disp":"57:47"},
			{"title":"Unpaired image translation and CycleGAN", "m":60, "s":37, "disp":"1:00:37"},
			{"title":"CycleGAN projects", "m":64, "s":08, "disp":"1:04:08"},
			{"title":"Object detection (YOLO) and dense captioning", "m":74, "s":52, "disp":"1:14:52"},
			{"title":"Image-to-text & text-to-image", "m":79, "s":16, "disp":"1:19:16"},
			{"title":"Installing dataset-utils & pix2pix/CycleGAN", "m":80, "s":54, "disp":"1:20:54"},
			{"title":"Extracting faces from a movie", "m":92, "s":32, "disp":"1:32:32"},
			{"title":"Making a pix2pix edge-to-photo dataset", "m":106, "s":50, "disp":"1:46:50"},
			{"title":"Training pix2pix on faces", "m":115, "s":17, "disp":"1:55:17"},
			{"title":"Scraping a dataset for CycleGAN", "m":124, "s":31, "disp":"2:04:31"},
			{"title":"Training CycleGAN to turn faces to clowns", "m":136, "s":0, "disp":"2:16:00"},
			{"title":"Installing densecap", "m":137, "s":51, "disp":"2:17:51"},
			{"title":"Short pix2pixHD tutorial & CycleGAN results", "m":141, "s":30, "disp":"2:21:30"},
			{"title":"Captioning images with densecap", "m":153, "s":01, "disp":"2:33:01"}
		],
		"summary": [
			"Image-to-image translation (pix2pix/CycleGAN)",
			"Scraping and preparing parallel datasets",
			"YOLO & dense captioning"
		],
		"pratical": [
			"<a href=\"https://ml4a.github.io/guides/Pix2Pix\">pix2pix tutorial</a>",
			"<a href=\"https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/\">CycleGAN + pix2pix</a>",
			"<a href=\"https://github.com/NVIDIA/pix2pixHD\">pix2pixHD</a>",
			"<a href=\"https://github.com/tkarras/progressive_growing_of_gans\">Progressive growing of GANS</a>"
		],
		"extra": [
			"<a href=\"https://magenta.tensorflow.org/\">Magenta</a>",
			"<a href=\"https://ml5js.org/docs/StyleTransfer\">ml5 style transfer</a>"
		]
	},
	{
		"date": "6 Nov 2018",
		"title": "Recurrent neural networks",
		"youtube_id": "HnBbsIXZ8Mo",
		"main":"/classes/itp-F18/08/",
		"slides": "https://storage.googleapis.com/ml4a.github.io/Slides_08/index.html",
		"thumbnail": "/images/classes/itp-F18/thumbnail_08.png",
		"bookmarks": [
			{"title":"Limitations of feedforward networks", "m":3, "s":10, "disp":"3:10"},
			{"title":"Recurrent neural networks", "m":5, "s":45, "disp":"5:45"},
			{"title":"RNN for text characters", "m":9, "s":47, "disp":"9:47"},
			{"title":"Sequence-to-sequence models", "m":14, "s":31, "disp":"14:31"},
			{"title":"Image-to-text", "m":17, "s":36, "disp":"17:36"},
			{"title":"Dense captioning", "m":25, "s":21, "disp":"25:21"},
			{"title":"Text-to-image", "m":28, "s":30, "disp":"28:30"},
			{"title":"im2txt tutorial & scenescoop", "m":32, "s":13, "disp":"32:13"},
			{"title":"LSTM", "m":48, "s":24, "disp":"48:24"},
			{"title":"char-rnn examples/hacks", "m":53, "s":32, "disp":"53:32"},
			{"title":"char-rnn tutorial", "m":78, "s":23, "disp":"1:18:23"},
			{"title":"Sketch-RNN", "m":91, "s":57, "disp":"1:31:57"},
			{"title":"Sketch-RNN tutorial", "m":106, "s":38, "disp":"1:46:38"},
			{"title":"Other RNN projects", "m":122, "s":02, "disp":"2:02:02"},
			{"title":"Attention, NTMs, and misc topics", "m":127, "s":23, "disp":"2:07:23"},
			{"title":"Sampling from char-rnn", "m":132, "s":06, "disp":"2:12:06"}
		],
		"summary": [
			"Recurrent networks & LSTMs",
			"Sequence-to-sequence applications",
			"im2txt, Sketch-RNN, char-rnn tutorials"
		],
		"pratical": [
			"<a href=\"https://github.com/tensorflow/magenta/tree/master/magenta/models/sketch_rnn\">Sketch-RNN</a>",
			"<a href=\"https://github.com/sherjilozair/char-rnn-tensorflow\">char-rnn-tensorflow</a>",
			"<a href=\"https://github.com/runwayml/models/tree/master/im2txt\">im2txt</a>"			
		],
		"extra": [
			"<a href=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/\">Understanding LSTMs</a>",
			"<a href=\"http://karpathy.github.io/2015/05/21/rnn-effectiveness/\">Unreasonable Effectiveness of RNNs</a>",
			"<a href=\"https://arxiv.org/abs/1704.03477\">A Neural Representation of Sketch Drawings</a>"			
		]
	},
	{
		"date": "13 Nov 2018",
		"title": "Music information retrieval, BIGGAN & GLOW",
		"youtube_id": "kIvyex9HH0Y",
		"main":"/classes/itp-F18/09/",
		"slides": "https://storage.googleapis.com/ml4a.github.io/Slides_09/index.html",
		"thumbnail": "/images/classes/itp-F18/thumbnail_09.png",
		"bookmarks": [
			{"title":"Flow and reversibility in generative models", "m":5, "s":22, "disp":"5:22"},
			{"title":"GLOW and face attribute manipulation", "m":9, "s":15, "disp":"9:15"},
			{"title":"GLOW tutorial", "m":16, "s":50, "disp":"16:50"},
			{"title":"Interpolations and random latent vectors", "m":30, "s":46, "disp":"30:46"},
			{"title":"BigGAN tutorial with pre-trained model", "m":39, "s":36, "disp":"39:36"},
			{"title":"Tasks in music information retrieval (MIR)", "m":50, "s":36, "disp":"50:36"},
			{"title":"Representations of music and audio", "m":57, "s":42, "disp":"57:42"},
			{"title":"Review of AudioClassifier & Audio t-SNE", "m":66, "s":55, "disp":"1:06:55"},
			{"title":"History of electronic & computer music", "m":71, "s":51, "disp":"1:11:51"},
			{"title":"Physical models and programmable audio", "m":83, "s":35, "disp":"1:23:35"},
			{"title":"GRUV and RNN-based audio modeling", "m":89, "s":05, "disp":"1:29:05"},
			{"title":"WaveNets, SampleRNN, Magenta NSynth", "m":92, "s":50, "disp":"1:32:50"},
			{"title":"Modeling symbolic music", "m":105, "s":54, "disp":"1:45:54"},
			{"title":"Content-based recommendation & iPod of the future", "m":115, "s":15, "disp":"1:55:15"}
		],
		"summary": [
			"GLOW and reversible generative models",
			"BigGAN tutorial",
			"Music information retrieval"
		],
		"pratical": [
			"<a href=\"https://github.com/genekogan/glow\">GLOW demo</a>",			
			"<a href=\"https://drive.google.com/open?id=1rqDwIddy0eunhhV8yrznG4SNiB5XWFJJ\">BigGAN demo</a>",			
			"<a href=\"http://ml4a.github.io/guides/AudioTSNEViewer/\">Audio t-SNE</a>",
			"<a href=\"http://ml4a.github.io/guides/AudioClassifier/\">AudioClassifier</a>",
		],
		"extra": [
			"<a href=\"https://blog.openai.com/glow/\">Glow: Better Reversible Generative Models</a>",			
			"<a href=\"https://medium.com/artists-and-machine-intelligence/neural-nets-for-generating-music-f46dffac21c0\">Neural Nets for Generating Music</a>"
		]
	},	
	{
		"date": "27 Nov 2018",
		"title": "Reinforcement Learning & Natural Language Processing",
		"youtube_id": "Pf_0IT5OjiI",
		"main":"/classes/itp-F18/10/",
		"slides": "https://storage.googleapis.com/ml4a.github.io/Slides_10/index.html",
		"thumbnail": "/images/classes/itp-F18/thumbnail_10.png",
		"bookmarks": [
			{"title":"Introduction to natural language processing (NLP)", "m":5, "s":40, "disp":"5:40"},
			{"title":"Why is NLP hard?", "m":10, "s":02, "disp":"10:02"},
			{"title":"Word embeddings", "m":12, "s":02, "disp":"12:02"},
			{"title":"Properties of word vectors", "m":21, "s":10, "disp":"21:10"},
			{"title":"Tutorial: universal sentence encoder", "m":28, "s":24, "disp":"28:24"},
			{"title":"Applications of sentence embeddings", "m":37, "s":21, "disp":"37:21"},
			{"title":"Machine translation", "m":43, "s":53, "disp":"43:53"},
			{"title":"Tutorial: Wikipedia latent semantic analysis (LSA)", "m":50, "s":47, "disp":"50:47"},
			{"title":"spaCy tutorial ", "m":60, "s":24, "disp":"1:00:24"},
			{"title":"Introduction to reinforcement learning (RL)", "m":66, "s":23, "disp":"1:06:23"},
			{"title":"The RL setup", "m":69, "s":07, "disp":"1:09:07"},
			{"title":"Examples and challenges of RL problems", "m":72, "s":03, "disp":"1:12:03"},
			{"title":"Deep Q-Networks for beating Atari games", "m":76, "s":22, "disp":"1:16:22"},
			{"title":"Applications to robotics and humanoid simulation", "m":86, "s":24, "disp":"1:26:24"},
			{"title":"Monte Carlo tree search (MCTS)", "m":91, "s":06, "disp":"1:31:06"},
			{"title":"Tic-tac-toe MCTS", "m":93, "s":57, "disp":"1:33:57"},
			{"title":"Introduction to Go and AlphaGo", "m":102, "s":18, "disp":"1:42:18"},
			{"title":"How AlphaGo improves MCTS", "m":110, "s":18, "disp":"1:50:18"},
			{"title":"AlphaGo vs. Lee Sedol", "m":115, "s":19, "disp":"1:55:19"},
			{"title":"AlphaGo Zero and discarding training data", "m":118, "s":40, "disp":"1:58:40"},
			{"title":"AlphaZero generalized", "m":125, "s":03, "disp":"2:05:03"},
			{"title":"AlphaZero plays chess and crushes Stockfish", "m":129, "s":55, "disp":"2:09:55"},
			{"title":"Curiosity-driven RL exploration ", "m":136, "s":26, "disp":"2:16:26"},
			{"title":"Practical resources for reinforcement learning", "m":138, "s":01, "disp":"2:18:01"}
		],
		"summary": [
			"Natural language processing",
			"Agent-environment systems & games",
			"AlphaGo and AlphaZero"
		],
		"pratical": [		
			"<a href=\"https://github.com/ml4a/ml4a-guides/tree/master/notebooks/q_learning.ipynb\">Q-learning</a>",
			"<a href=\"https://github.com/ml4a/ml4a-guides/tree/master/notebooks/deep_q_networks.ipynb\">Deep Q-Networks</a>",
			"<a href=\"https://github.com/ml4a/ml4a-guides/tree/master/notebooks/text-retrieval.ipynb\">Latent semantic analysis</a> / <a href=\"http://genekogan.com/works/wiki-tSNE\">Text t-SNE</a>",
			"<a href=\"https://ml5js.org/docs/Word2vec\">ml5 word vectors</a>",
		],
		"extra": [
			"<a href=\"https://unity3d.com/machine-learning\">Unity Agents</a>",
			"<a href=\"https://gym.openai.com/\">OpenAI Gym</a>",
			"<a href=\"http://karpathy.github.io/2016/05/31/rl/\">Pong from Pixels</a>"
		]
	},
	{
		"date": "4 Dec 2018",
		"title": "Autonomous artificial artist",
		"youtube_id": "Dqudnjj5wSo",
		"main":"/classes/itp-F18/11/",
		"slides": "https://storage.googleapis.com/ml4a.github.io/Slides_11/index.html",
		"thumbnail": "/images/classes/itp-F18/thumbnail_11.png",
		"bookmarks": [		
			{"title":"Why AI and decentralization are relevant to each other", "m":5, "s":21, "disp":"5:21"},
			{"title":"Quick primer on cryptography", "m":18, "s":35, "disp":"18:35"},
			{"title":"Applications of public key cryptography", "m":27, "s":55, "disp":"27:55"},
			{"title":"Hash functions and proof-of-work", "m":31, "s":39, "disp":"31:39"},
			{"title":"Peer-to-peer networks", "m":38, "s":37, "disp":"38:37"},
			{"title":"Problems with the web today & IPFS project", "m":46, "s":08, "disp":"46:08"},
			{"title":"How Bitcoin works", "m":54, "s":45, "disp":"54:45"},
			{"title":"Blockchain-secured assets & second-generation applications", "m":68, "s":55, "disp":"1:08:55"},
			{"title":"Smart contracts & Ethereum", "m":73, "s":04, "disp":"1:13:04"},
			{"title":"Applications of smart contracts", "m":76, "s":45, "disp":"1:16:45"},
			{"title":"Decentralized autonomous organizations", "m":83, "s":42, "disp":"1:23:42"},
			{"title":"Cryptoeconomics", "m":87, "s":41, "disp":"1:27:41"},
			{"title":"Tokens and ICOs", "m":88, "s":50, "disp":"1:28:50"},
			{"title":"Continuous organizations and curved bonding", "m":97, "s":09, "disp":"1:37:09"},
			{"title":"Curation markets", "m":109, "s":14, "disp":"1:49:14"},
			{"title":"Governance as curation and liquid democracy", "m":116, "s":22, "disp":"1:56:22"},
			{"title":"Problems with centralized machine learning", "m":119, "s":48, "disp":"1:59:48"},
			{"title":"Federated learning, homomorphic encryption, and OpenMined", "m":123, "s":35, "disp":"2:03:35"},
			{"title":"Pros and cons of decentralized machine learning setups", "m":135, "s":36, "disp":"2:15:36"},
			{"title":"Numerai and Ocean protocol", "m":138, "s":45, "disp":"2:18:45"},
			{"title":"The idea of an autonomous artist", "m":145, "s":56, "disp":"2:25:56"},
			{"title":"Crypto-collectibles, ArtDAOs, and other related ideas", "m":147, "s":27, "disp":"2:27:27"},
			{"title":"Components of an autonomous artificial artist", "m":153, "s":14, "disp":"2:33:14"}
		],
		"summary": [
			"Cryptography, peer-to-peer networks, DAOs",
			"Curation markets and cryptoeconomics",
			"Decentralizing machine learning"
		],
		"pratical": [
		],
		"extra": [
			"<a href=\"https://www.openmined.org/\">OpenMined</a>",
			"<a href=\"https://github.com/artonomous/artonomous-mvp\">Artonomous</a>"
		]
	},
	{
		"date": "11 Dec 2018",
		"title": "Final presentations",
		"thumbnail": "",
		"bookmarks": [],
		"summary": [
		],
		"pratical": [
		],
		"extra": [
		]
	}
],	




"misc":[{
	"youtube_id": "mbkbMvMxLmg",
	"title": "From principal components to puppyslugs",
	"date": "6/21/2017",
	"main":"/classes/misc/01/",
	"thumbnail": "/images/classes/misc/thumbnail_01.png",
	"dropbox": "",
	"bookmarks": [
		{"title":"Generative models", "m":4, "s":10, "disp":"4:10"},
		{"title":"Why sampling images is hard", "m":5, "s":58, "disp":"5:58"},
		{"title":"Principal component analysis", "m":9, "s":57, "disp":"9:57"},
		{"title":"Eigenfaces", "m":15, "s":15, "disp":"15:15"},
		{"title":"Neural networks", "m":29, "s":28, "disp":"29:28"},
		{"title":"Representation learning and feature extraction", "m":48, "s":26, "disp":"48:26"},
		{"title":"Autoencoders", "m":61, "s":31, "disp":"1:01:31"},
		{"title":"Generative adversarial networks", "m":67, "s":31, "disp":"1:07:31"},
		{"title":"BEGAN, InfoGAN, DiscoGAN, StackGAN, ArtGAN", "m":77, "s":44, "disp":"1:17:44"},
		{"title":"Deep generator networks", "m":81, "s":50, "disp":"1:21:50"},
		{"title":"Conditional GANs (pix2pix)", "m":86, "s":25, "disp":"1:26:25"},
		{"title":"CycleGANs, horse2zebra", "m":94, "s":27, "disp":"1:34:27"},
		{"title":"Skip-thought vectors and WaveNets", "m":97, "s":11, "disp":"1:37:11"},
		{"title":"Class synthesis, deepdream, and puppyslugs", "m":101, "s":08, "disp":"1:41:08"}
	],
	"summary": [
		"Generative modeling of images",
		"Principal component analysis and Eigenfaces",
		"Autoencoders, generative adversarial networks, pix2pix",
		"Deepdream, class synthesis, and puppyslugs",
		"Taught by <a href=\"https://twitter.com/genekogan\">@genekogan</a>"
	]}
],
"opendot":[{
	"youtube_id": "yHOmMCY589Y",
	"title": "Why machine learning for artists",
	"date": "11/21/2016",
	"main":"/classes/opendot/01/",
	"thumbnail": "/images/classes/opendot/thumbnail_01.png",
	"dropbox": "https://www.dropbox.com/s/fxz3pbyoj2se91j/opendot_11.21_01.mp4?dl=1",
	"bookmarks": [
		{"title":"Introduction, about the class", "m":0, "s":0, "disp":"0:00"},
		{"title":"Neither democrats nor dictators; Why ML for artists", "m":8, "s":4, "disp":"8:04"},
		{"title":"Early history of AI & ML through 1980s", "m":15, "s":37, "disp":"15:37"},
		{"title":"Emergence of deep learning", "m":31, "s":48, "disp":"31:48"},
		{"title":"Critical issues in AI", "m":52, "s":20, "disp":"52:20"},
		{"title":"Neural networks and the brain analogy", "m":66, "s":36, "disp":"1:06:36"},
		{"title":"Showcase of art projects using ML", "m":68, "s":59, "disp":"1:08:59"},
		{"title":"Resources + ml4a.github.io", "m":86, "s":46, "disp":"1:26:46"}
	],
	"summary": [
		"Overview of the class, why ML for artists",
		"Micro-history of AI, machine learning, and deep learning",
		"Some examples of artistic ML works",
		"Resources + ml4a.github.io"
	]	
},{
	"youtube_id": "sAs0YUwxAoY",
	"title": "Neural networks",
	"date": "11/21/2016",
	"main":"/classes/opendot/02/",
	"thumbnail": "/images/classes/opendot/thumbnail_02.png",
	"dropbox": "https://www.dropbox.com/s/x1mozy0kxn75g2m/opendot_11.21_02.mp4?dl=1",
	"bookmarks": [
		{"title":"Supervised and unsupervised machine learning", "m":0, "s":0, "disp":"0:00"},
		{"title":"Introduction to neurons and neural networks", "m":2, "s":48, "disp":"2:48"},
		{"title":"An example: classifying images of handwritten digits", "m":14, "s":23, "disp":"14:23"},
		{"title":"Visualizing the weights during training", "m":19, "s":46, "disp":"19:46"},
		{"title":"The big picture: generalizing supervised learning and performance applications", "m":35, "s":9, "disp":"35:09"}
	],
	"summary": [
		"How neural networks work",
		"Visualizing neural networks during training",
		"General applications of neural nets"
	]
},{
	"youtube_id": "Y3UuIER66FQ",
	"title": "Real-time ML for performance",
	"date": "11/21/2016",
	"main":"/classes/opendot/03/",
	"thumbnail": "/images/classes/opendot/thumbnail_03.png",
	"dropbox": "https://www.dropbox.com/s/qlk1v6db0r6yxd1/opendot_11.21_03.mp4?dl=1",
	"bookmarks": [
		{"title":"Machine learning for real-time performance, music, and art", "m":0, "s":0, "disp":"0:00"},
		{"title":"Basic Wekinator + Processing app", "m":10, "s":19, "disp":"10:19"},
		{"title":"Sound example: controlling an FM Synth", "m":56, "s":37, "disp":"56:37"},
		{"title":"FaceOSC controlling FM Synth", "m":73, "s":43, "disp":"1:13:43"},
		{"title":"Putting it altogether! FaceOSC controlling Ableton", "m":86, "s":58, "disp":"1:26:58"},
		{"title":"FaceOSC controlling AudioUnits", "m":89, "s":25, "disp":"1:29:25"}
	],
	"summary": [
		"Basic Processing + Wekinator application",
		"Using neural nets to control audio synths",
		"Making music with a face tracker"
	]
},{
	"youtube_id": "vCIDk-bS_zQ",
	"title": "Convolutional neural networks + t-SNE",
	"date": "11/22/2016",
	"main":"/classes/opendot/04/",
	"thumbnail": "/images/classes/opendot/thumbnail_04.png",
	"dropbox": "https://www.dropbox.com/s/r6ud3p7ry0dmcak/opendot_11.22_01.mp4?dl=1",
	"bookmarks": [
		{"title":"Recap of day 1", "m":0, "s":0, "disp":"0:00"},
		{"title":"Limitations of ordinary neural nets", "m":5, "s":46, "disp":"5:46"},
		{"title":"How convolutional neural networks work", "m":11, "s":13, "disp":"11:13"},
		{"title":"Convnet demo", "m":24, "s":27, "disp":"24:27"},
		{"title":"Probing and visualizing convnet activations", "m":38, "s":22, "disp":"38:22"},
		{"title":"Transfer learning and reverse image search demo", "m":44, "s":50, "disp":"44:50"},
		{"title":"Organizing and visualizing image sets with t-SNE", "m":53, "s":51, "disp":"53:51"},
		{"title":"Visualizing text documents with t-SNE", "m":57, "s":7, "disp":"57:07"},
		{"title":"Audio t-SNE", "m":59, "s":28, "disp":"59:28"},
		{"title":"Assigning image t-SNEs to grids", "m":61, "s":2, "disp":"1:01:02"}
	],
	"summary": [
		"How convnets work",
		"Activations are useful; reverse image search",
		"t-SNE for organizing image collections",
		"t-SNE in text and audio domains"
	]
},{
	"youtube_id": "L3OOWqir-9g",
	"title": "Applications of t-SNE",
	"date": "11/23/2016",
	"main":"/classes/opendot/05/",
	"thumbnail": "/images/classes/opendot/thumbnail_05.png",
	"dropbox": "https://www.dropbox.com/s/535vx1ua4jox605/opendot_11.23_01.mp4?dl=1",
	"bookmarks": [
		{"title":"About Image-to-Image translation paper", "m":0, "s":0, "disp":"0:00"},
		{"title":"Make your own live image classifier with ConvnetOSC and Wekinator", "m":11, "s":10, "disp":"11:10"},
		{"title":"Make your own Audio t-SNE in openFrameworks", "m":22, "s":12, "disp":"22:12"},
		{"title":"Bohemian Rhapsody segmented t-SNE", "m":33, "s":0, "disp":"33:00"},
		{"title":"Class discussion of techniques, style transfer in other domains", "m":35, "s":22, "disp":"35:22"}
	],
	"summary": [
		"Discussion of image-to-image translation",
		"Transfer learning with convnets",
		"How to make audio t-SNEs"
	]
},{
	"youtube_id": "E2r3_oddwEc",
	"title": "Applications of deep learning",
	"date": "11/23/2016",
	"main":"/classes/opendot/06/",
	"thumbnail": "/images/classes/opendot/thumbnail_06.png",
	"dropbox": "https://www.dropbox.com/s/4730zse93xy3o22/opendot_11.23_02.mp4?dl=1",
	"bookmarks": [
		{"title":"Generative visual applications of convnets", "m":0, "s":0, "disp":"0:00"},
		{"title":"Class visualization and deepdream", "m":2, "s":28, "disp":"2:28"},
		{"title":"Deepdream implementations and code", "m":10, "s":14, "disp":"10:14"},
		{"title":"Style transfer & examples", "m":12, "s":12, "disp":"12:12"},
		{"title":"Video style transfer", "m":21, "s":29, "disp":"21:29"},
		{"title":"Special cases of style transfer and image-to-image mapping", "m":26, "s":6, "disp":"26:06"},
		{"title":"Recurrent neural networks and LSTMs", "m":32, "s":28, "disp":"32:28"},
		{"title":"Dense captioning and sequence-based applications", "m":46, "s":50, "disp":"46:50"}
	],
	"summary": [
		"Generative images; Deepdream and style transfer",
		"Image to image mapping",
		"LSTMs, text generation, and dense captioning"
	]
}],
"neural-aesthetic":[{
	"youtube_id": "kc2XOUEyhDM",
	"title": "Machine learning for artists",
	"date": "7/4/2016",
	"main":"/classes/neural-aesthetic/01/",
	"thumbnail": "/images/classes/neural-aesthetic/thumbnail_01.png",
	"dropbox": "https://www.dropbox.com/s/vz5qlb6ee2ffjhp/neural%20aesthetic%20%40%20schoolofma%20--%2001%20machine%20learning%20for%20artists.mp4?dl=1",
	"bookmarks": [
		{"title":"Introduction, policies, syllabus, resources + ml4a", "m":0, "s":0, "disp":"0:00"},
		{"title":"Fun with Meapsoft and music information retrieval", "m":43, "s":03, "disp":"43:03"},
		{"title":"What is machine learning?", "m":58, "s":42, "disp":"58:42"},
		{"title":"AI hype cycles", "m":70, "s":23, "disp":"1:10:23"},
		{"title":"Objectives of AI (HAL in 2001: A Space Odyssey)", "m":76, "s":45, "disp":"1:16:45"},
		{"title":"ML for media art, Wekinator", "m":81, "s":04, "disp":"1:21:04"},
		{"title":"Deep learning art applications: Deepdream and Style transfer", "m":84, "s":22, "disp":"1:24:22"},
		{"title":"Survey of recent artworks, and alt-AI exhibition pieces", "m":93, "s":37, "disp":"1:33:37"}
	],
	"summary": [
		"Hype and history of machine learning",
		"Overview of neural networks",
		"Applications of neural networks",
		"Survey of recent ML-inspired artworks"
	]	
},{
	"youtube_id": "ED-_-JzvvFk",
	"title": "Neural networks",
	"date": "7/5/2016",
	"main":"/classes/neural-aesthetic/02/",
	"thumbnail": "/images/classes/neural-aesthetic/thumbnail_02.png",
	"dropbox": "https://www.dropbox.com/s/ln5ylmvfm6xku6q/neural%20aesthetic%20%40%20schoolofma%20--%2002%20neural%20networks.mp4?dl=1",
	"bookmarks": [
		{"title":"From biological to artificial neurons", "m":1, "s":32, "disp":"1:32"},
		{"title":"Defining artificial neurons and activation functions", "m":13, "s":10, "disp":"13:10"},
		{"title":"A simple neural network and forward pass", "m":23, "s":58, "disp":"23:58"},
		{"title":"Classifying images of handwritten digits", "m":31, "s":48, "disp":"31:48"},
		{"title":"Visualizing the weights of a neural net", "m":38, "s":21, "disp":"38:21"},
		{"title":"Overview of convolutional neural networks", "m":60, "s":38, "disp":"1:00:38"},
		{"title":"End-to-end demo of a convnet", "m":99, "s":29, "disp":"1:39:29"},
		{"title":"Interpreting convnets and visual applications", "m":130, "s":13, "disp":"2:10:13"}
	],
	"summary": [
		"How neural networks work",
		"Overview of convolutional neural nets",
		"Visualizing convnets"
	]
},{
	"youtube_id": "euMXlFJlSTQ",
	"title": "Convolutional neural networks + t-SNE",
	"date": "7/6/2016",
	"main":"/classes/neural-aesthetic/03/",
	"thumbnail": "/images/classes/neural-aesthetic/thumbnail_03.png",
	"dropbox": "https://www.dropbox.com/s/bcqcmgh4uegek8d/neural%20aesthetic%20%40%20schoolofma%20--%2003%20convnets%20%2B%20t-SNE.mp4?dl=1",
	"bookmarks": [
		{"title":"Review of neural networks", "m":9, "s":45, "disp":"9:45"},
		{"title":"Shortcomings of ordinary neural nets", "m":24, "s":16, "disp":"24:16"},
		{"title":"Convolutional layers", "m":36, "s":13, "disp":"36:13"},
		{"title":"Visualizing what convnet layers learn", "m":51, "s":07, "disp":"51:07"},
		{"title":"Deepdream, style transfer, variational autoencoders", "m":54, "s":12, "disp":"54:12"},
		{"title":"Overview of recurrent neural nets", "m":71, "s":21, "disp":"1:11:21"},
		{"title":"t-SNE embedding of images in 2d", "m":84, "s":58, "disp":"1:24:58"},
		{"title":"Embedding text articles with t-SNE (e.g. political ideologies)", "m":102, "s":52, "disp":"1:42:52"},
		{"title":"Audio t-SNE: embedding audio samples in 2d", "m":113, "s":22, "disp":"1:53:22"},
		{"title":"Tutorial: how to make a t-SNE of images", "m":140, "s":19, "disp":"2:20:19"}
	],
	"summary": [
		"How convolutional neural networks work",
		"Overview of recurrent neural networks",
		"Embedding images, text, and audio with t-SNE",
		"Image t-SNE tutorial"
	]
},{
	"youtube_id": "l8lJ8hxHyyc",
	"title": "Trolley problem, discussion {poor audio}",
	"date": "7/7/2016",
	"main":"/classes/neural-aesthetic/04/",
	"thumbnail": "/images/classes/neural-aesthetic/thumbnail_04.png",
	"dropbox": "https://www.dropbox.com/s/f1sr8ogeih5zeuq/neural%20aesthetic%20%40%20schoolofma%20--%2004%20trolley%20problem%2C%20ethics.mp4?dl=1",
	"bookmarks": [
		{"title":"Robots as our \"children\"", "m":0, "s":0, "disp":"0:00"},
		{"title":"The trolley problem", "m":17, "s":33, "disp":"17:33"},
		{"title":"Descartes: animals are machines", "m":46, "s":10, "disp":"46:10"},
		{"title":"Review of week 1", "m":50, "s":50, "disp":"50:50"},
		{"title":"Assignment + introduction to Wekinator, review resources", "m":64, "s":50, "disp":"1:04:50"},
		{"title":"Democratizing AI research (Keras) + discussion", "m":79, "s":06, "disp":"1:19:06"}
	],
	"summary": [
		"Trolley problem {poor audio}",
		"Ethical dilemmas and critical issues in AI",
		"Democratizing AI research"
	]
},{
	"youtube_id": "JVD7MudjF10",
	"title": "Wekinator, real-time applications of neural nets",
	"date": "7/18/2016",
	"main":"/classes/neural-aesthetic/05/",
	"thumbnail": "/images/classes/neural-aesthetic/thumbnail_09.png",
	"dropbox": "https://www.dropbox.com/s/enudiynnm9kqrt7/neural%20aesthetic%20%40%20schoolofma%20--%2009%20wekinator%20examples%2C%20modules.mp4?dl=1",
	"bookmarks": [
		{"title":"The big picture: neural nets map volumes", "m":0, "s":0, "disp":"0:00"},
		{"title":"Real-time and interactive media-driven uses of neural nets", "m":9, "s":34, "disp":"9:34"},
		{"title":"Controlling audio units with OSC", "m":22, "s":20, "disp":"22:20"},
		{"title":"Transfer learning from trained convnet", "m":31, "s":32, "disp":"31:32"},
		{"title":"KinectOSC -> Wekinator -> Processing sketch", "m":36, "s":58, "disp":"36:58"},
		{"title":"Microphone audio -> Wekinator -> openFrameworks glitchy visuals", "m":69, "s":26, "disp":"1:09:26"},
		{"title":"Making your own image classifier via ConvnetOSC -> Wekinator", "m":144, "s":0, "disp":"2:24:00"}
	],
	"summary": [
		"Real-time interaction with ML via Wekinator",
		"Wekinator examples and modules",
		"Transfer learning with ConvnetOSC and Wekinator"
	]
},{
	"youtube_id": "--eZESmJBXQ",
	"title": "Applications of convnets",
	"date": "7/19/2016",
	"main":"/classes/neural-aesthetic/06/",
	"thumbnail": "/images/classes/neural-aesthetic/thumbnail_10.png",
	"dropbox": "https://www.dropbox.com/s/lomjxbz3vokmfhl/neural%20aesthetic%20%40%20schoolofma%20--%2010%20convnet%20applications.mp4?dl=1",
	"bookmarks": [
		{"title":"Review how convnets work", "m":0, "s":0, "disp":"0:00"},
		{"title":"Visualizing and interpreting learned features", "m":21, "s":54, "disp":"21:54"},
		{"title":"Why the activations are valuable", "m":27, "s":46, "disp":"27:46"},
		{"title":"Image embeddings with t-SNE", "m":36, "s":38, "disp":"36:38"},
		{"title":"Class visualization and Deepdream", "m":53, "s":37, "disp":"53:37"},
		{"title":"Style transfer", "m":85, "s":12, "disp":"1:25:12"},
		{"title":"Special cases of style transfer: video, real-time, colorless", "m":102, "s":14, "disp":"1:42:14"},
		{"title":"Image analogies, neural-doodle, super-resolution, assistive", "m":109, "s":10, "disp":"1:49:10"},
		{"title":"Colorizing black & white images", "m":115, "s":27, "disp":"1:55:27"},
		{"title":"Deep convolutional generative adversarial networks", "m":117, "s":34, "disp":"1:57:34"},
		{"title":"Tutorial: neural-style in terminal instance", "m":123, "s":01, "disp":"2:03:01"}
	],
	"summary": [
		"Visualizing convnets and interpreting activations",
		"Deepdream, style transfer, and other convnet visual applications",
		"neural-style tutorial"
	]
},{
	"youtube_id": "vrBh9Nd1QP4",
	"title": "Recurrent neural networks",
	"date": "7/20/2016",
	"main":"/classes/neural-aesthetic/07/",
	"thumbnail": "/images/classes/neural-aesthetic/thumbnail_11.png",
	"dropbox": "https://www.dropbox.com/s/2gv770ublilrn3k/neural%20aesthetic%20%40%20schoolofma%20--%2011%20recurrent%20neural%20networks.mp4?dl=1",
	"bookmarks": [
		{"title":"Feedforward vs. recurrent", "m":0, "s":0, "disp":"0:00"},
		{"title":"RNNs predicting characters", "m":7, "s":31, "disp":"7:31"},
		{"title":"Text generation: Shakespeare, XML, LaTeX, recipes, TED, dictionaries, sci-fi, emojis, Trump", "m":12, "s":18, "disp":"12:18"},
		{"title":"Architectures: sequence to sequence, captioning images", "m":32, "s":0, "disp":"32:00"},
		{"title":"Dense-captioning images", "m":46, "s":41, "disp":"46:41"},
		{"title":"Sequence to unit and misc applications", "m":52, "s":42, "disp":"52:42"},
		{"title":"Sound/music: sequencing audio and MIDI", "m":61, "s":50, "disp":"1:01:50"}
	],
	"summary": [
		"How recurrent neural networks work",
		"Applications and artworks with RNNs and LSTMs",
		"Dense captioninig and advanced architectures"
	]
},{
	"youtube_id": "QES5UgIO9B4",
	"title": "Game AI & reinforcement learning",
	"date": "7/21/2016",
	"main":"/classes/neural-aesthetic/08/",
	"thumbnail": "/images/classes/neural-aesthetic/thumbnail_12.png",
	"dropbox": "https://www.dropbox.com/s/g3neicmofe8r65y/neural%20aesthetic%20%40%20schoolofma%20--%2012%20reinforcement%20learning%20%2B%20game%20AI.mp4?dl=1",
	"bookmarks": [
		{"title":"The Glass Bead Game + DeepMind", "m":0, "s":0, "disp":"0:00"},
		{"title":"What is reinforcement learning?", "m":9, "s":42, "disp":"9:42"},
		{"title":"Learning how to play Atari games", "m":17, "s":10, "disp":"17:10"},
		{"title":"Convnets controlling joysticks", "m":33, "s":07, "disp":"33:07"},
		{"title":"RL IRL: motor learning to balance a pole", "m":46, "s":50, "disp":"46:50"},
		{"title":"Games and tree search: Tic tac toe + Chess", "m":49, "s":22, "disp":"49:22"},
		{"title":"Putting it all together: AlphaGo", "m":62, "s":32, "disp":"1:02:32"}
	],
	"summary": [
		"Reinforcement learning",
		"Convnets playing Atari games",
		"DeepBlue and AlphaGo"
	]
}],
"itp-S16":[{
	"youtube_id": "z0bynQjEpII",
	"title": "Introduction, neural networks",
	"date": "3/24/2016",
	"main":"/classes/itp-S16/01/",
	"thumbnail": "/images/classes/itp-S16/thumbnail_01.png",
	"dropbox": "https://www.dropbox.com/s/jkgo87c2fe6zcmr/ml4a%20%40%20itp-nyu%20--%2001%20introduction%2C%20neural%20networks.mp4?dl=1",
	"bookmarks": [
		{"title":"Introduction", "m":0, "s":0, "disp":"0:00"}, 
		{"title":"Machine learning and neural networks", "m":11, "s":03, "disp":"11:03"}, 
		{"title":"Demo forward pass and MNIST", "m":23, "s":45, "disp":"23:45"}, 
		{"title":"Visualizing the weights", "m":52, "s":17, "disp":"52:17"}, 
		{"title":"MNIST/CIFAR confusion matrix", "m":62, "s":50, "disp":"1:02:50"}, 
		{"title":"Convolutional neural network demo", "m":81, "s":52, "disp":"1:21:52"}, 
		{"title":"Applications of convnets", "m":103, "s":58, "disp":"1:43:58"}
	],
	"summary": [
		"Introduction to machine learning",
		"The whole class in 1 hour",
		"Introduction to neural networks"
	]	
},{
	"youtube_id": "lKmkt5LEDS8",
	"title": "Applications, Wekinator",
	"date": "3/31/2016",
	"main":"/classes/itp-S16/02/",
	"thumbnail": "/images/classes/itp-S16/thumbnail_02.png",
	"dropbox": "https://www.dropbox.com/s/6gg635cbxazsp56/ml4a%20%40%20itp-nyu%20--%2002%20applications%2C%20wekinator.mp4?dl=1",
	"bookmarks": [
		{"title":"Review of neural networks", "m":7, "s":50, "disp":"7:50"}, 
		{"title":"Walkthrough of a practical ML experiment with ofxLearn", "m":10, "s":47, "disp":"10:47"}, 
		{"title":"Code resources for setting up ordinary neural nets", "m":22, "s":15, "disp":"22:15"}, 
		{"title":"Intro to Wekinator", "m":28, "s":12, "disp":"28:12"}, 
		{"title":"Basic walkthrough and simple example", "m":37, "s":30, "disp":"37:30"}, 
		{"title":"More complex example w/ FaceOSC", "m":64, "s":52, "disp":"1:04:52"}, 
		{"title":"FaceTracker -> happy/sad colors", "m":73, "s":26, "disp":"1:13:26"}, 
		{"title":"Leap Motion input", "m":86, "s":27, "disp":"1:26:27"}, 
		{"title":"Classification of webcam pixels", "m":101, "s":0, "disp":"1:41:00"}, 
		{"title":"Dynamic time warping with bark coefficients", "m":117, "s":24, "disp":"1:57:24"}, 
		{"title":"Critical reading", "m":126, "s":39, "disp":"2:06:39"}
	],
	"summary": [
		"Practical resources for simple neural nets",
		"Implementing neural nets in openFrameworks",
		"Introduction to Wekinator"
	]
},{
	"youtube_id": "z6k_RMKExlQ",
	"title": "Convolutional neural networks",
	"date": "4/7/2016",
	"main":"/classes/itp-S16/03/",
	"thumbnail": "/images/classes/itp-S16/thumbnail_03.png",
	"dropbox": "https://www.dropbox.com/s/9beww4qyq2nyqhe/ml4a%20%40%20itp-nyu%20--%2003%20convolutional%20neural%20networks.mp4?dl=1",
	"bookmarks": [
		{"title":"[Nick Hubbard] Trolley problem", "m":8, "s":15, "disp":"8:15"}, 
		{"title":"'Animals are machines' - Rene Descartes", "m":20, "s":30, "disp":"20:30"}, 
		{"title":"Wekinator custom outputs + Ableton", "m":25, "s":10, "disp":"25:10"}, 
		{"title":"Wekinator + Audio Units (mac)", "m":40, "s":48, "disp":"40:48"}, 
		{"title":"Wekinator + audioreactive Jitter", "m":43, "s":17, "disp":"43:17"}, 
		{"title":"Kinect + gesture recognition options", "m":52, "s":38, "disp":"52:38"}, 
		{"title":"Limitations of ordinary neural nets", "m":63, "s":16, "disp":"1:03:16"}, 
		{"title":"Convolutional and pooling layers", "m":84, "s":51, "disp":"1:24:51"}, 
		{"title":"Convnets: the whole pipeline", "m":109, "s":24, "disp":"1:49:24"}
	],
	"summary": [
		"Critical issues in AI",
		"More advanced Wekinator examples",
		"Introduction to convolutional neural networks"
	]
},{
	"youtube_id": "V0glxY6fHTY",
	"title": "Convnet applications",
	"date": "4/14/2016",
	"main":"/classes/itp-S16/04/",
	"thumbnail": "/images/classes/itp-S16/thumbnail_04.png",
	"dropbox": "https://www.dropbox.com/s/taaxxu6ebth78ay/ml4a%20%40%20itp-nyu%20--%2004%20convnet%20applications.mp4?dl=1",
	"bookmarks": [
		{"title":"Stranger Visions and reconstructing data", "m":8, "s":10, "disp":"8:10"},
		{"title":"Reviewing convnets", "m":13, "s":34, "disp":"13:34"},
		{"title":"Interpreting and visualizing activations", "m":40, "s":0, "disp":"40:00"},
		{"title":"Occlusion demo, localization/compression, deconvolution", "m":50, "s":12, "disp":"50:12"},
		{"title":"Image synthesis and Deepdream", "m":69, "s":53, "disp":"1:09:53"},
		{"title":"Style transfer", "m":91, "s":03, "disp":"1:31:03"},
		{"title":"Transfer learning (Convnet -> Wekinator)", "m":108, "s":20, "disp":"1:48:20"},
		{"title":"t-SNE on convnet activations (and text)", "m":117, "s":20, "disp":"1:57:20"}
	],
	"summary": [
		"Interpreting and visualizing convnet activations",
		"Image class synthesis, deepdream, style transfer",
		"Transfer learning",
		"t-SNE for visualizing images, text"
	]
},{
	"youtube_id": "u18NVUvSEsU",
	"title": "Recurrent neural networks",
	"date": "4/21/2016",
	"main":"/classes/itp-S16/05/",
	"thumbnail": "/images/classes/itp-S16/thumbnail_05.png",
	"dropbox": "https://www.dropbox.com/s/ap5tqz56n0m1fu0/ml4a%20%40%20itp-nyu%20--%2005%20recurrent%20neural%20networks.mp4?dl=1",
	"bookmarks": [
		{"title":"Review feedforward neural networks", "m":2, "s":19, "disp":"2:19"},
		{"title":"Feedforward vs. recurrence", "m":7, "s":38, "disp":"7:38"},
		{"title":"How recurrent neural nets work", "m":9, "s":06, "disp":"9:06"},
		{"title":"Training RNNs on text (character sequences)", "m":11, "s":32, "disp":"11:32"},
		{"title":"RNNs and sequence-to-sequence", "m":20, "s":05, "disp":"20:05"},
		{"title":"Image captioning", "m":22, "s":25, "disp":"22:25"},
		{"title":"Advanced architectures and applications", "m":28, "s":12, "disp":"28:12"},
		{"title":"Tutorial: text generation via torch-rnn", "m":34, "s":20, "disp":"34:20"},
		{"title":"Tutorial: style transfer via neural-style", "m":63, "s":31, "disp":"1:03:31"}
	],
	"summary": [
		"How recurrent neural networks work",
		"Applications, architectures, and case studies of RNNs",
		"Tutorial: generating text",
		"Tutorial: style transfer"
	]
},{
	"youtube_id": "CRM7HYZYHvo",
	"title": "Game AI and deep reinforcement learning",
	"date": "4/28/2016",
	"main":"/classes/itp-S16/06/",
	"thumbnail": "/images/classes/itp-S16/thumbnail_06.png",
	"dropbox": "https://www.dropbox.com/s/llzkkc9fj926qlr/ml4a%20%40%20itp-nyu%20--%2006%20reinforcement%20learning%2C%20games%2C%20generative%20models.mp4?dl=1",
	"bookmarks": [
		{"title":"The whole class \"in 10 minutes\"", "m":14, "s":33, "disp":"14:33"},
		{"title":"Autoencoders", "m":43, "s":03, "disp":"43:03"},
		{"title":"Generative adversarial networks", "m":53, "s":58, "disp":"53:58"},
		{"title":"Game AI + reinforcement learning", "m":64, "s":59, "disp":"1:04:59"},
		{"title":"Convnets mastering atari games", "m":74, "s":51, "disp":"1:14:51"},
		{"title":"States, actions, and rewards, Q-learning", "m":88, "s":52, "disp":"1:28:52"},
		{"title":"Super mario craziness, computer tic-tac-toe", "m":100, "s":48, "disp":"1:40:48"},
		{"title":"Computer chess: how DeepBlue works", "m":109, "s":59, "disp":"1:49:59"},
		{"title":"Computer go: how AlphaGo works", "m":124, "s":05, "disp":"2:04:05"}
	],
	"summary": [
		"Generative models (autoencoders + GANs)",
		"Game AI + reinforcement learning",
		"Convnets mastering atari games",
		"Computer chess & computer go, how AlphaGo works"
	]
},{
	"youtube_id": "7wXnI2poifI",
	"title": "The Neural Aesthetic",
	"date": "6/4/2016",
	"main":"/classes/itp-S16/07/",
	"thumbnail": "/images/classes/itp-S16/thumbnail_07.png",
	"dropbox": "https://www.dropbox.com/s/ilqrbn8i4n66sv7/ml4a%20%40%20itp-nyu%20--%20the%20neural%20aesthetic%206.4.mp4?dl=1",
	"bookmarks": [
		{"title":"ML circa 2010 + music info retrieval", "m":11, "s":10, "disp":"11:10"},
		{"title":"From perceptrons to deep neural nets", "m":27, "s":25, "disp":"27:25"},
		{"title":"Convolutional neural networks", "m":64, "s":12, "disp":"1:04:12"},
		{"title":"Convolution demo end to end", "m":81, "s":20, "disp":"1:21:20"},
		{"title":"Visualizing activations and synthesizing classes", "m":94, "s":23, "disp":"1:34:23"},
		{"title":"Deepdream and style transfer", "m":105, "s":35, "disp":"1:45:35"},
		{"title":"Video style transfer with optical flow", "m":124, "s":08, "disp":"2:04:08"},
		{"title":"t-SNE on images and text", "m":132, "s":41, "disp":"2:12:41"},
		{"title":"Generative models + DCGANs", "m":142, "s":19, "disp":"2:22:19"},
		{"title":"Recurrent neural networks, game AI + RL", "m":151, "s":39, "disp":"2:31:39"},
		{"title":"Recent developments + real-time style transfer", "m":160, "s":48, "disp":"2:40:48"}
	],
	"summary": [
		"Theory of neural networks",
		"Applications of convnets (deepdream, style transfer, etc)",
		"Survey of sub-topics and practical resources",
		"ml4a and how to keep up with the field"
	]
}]
}